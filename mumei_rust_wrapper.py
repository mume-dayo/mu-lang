#!/usr/bin/env python3
"""
Mumei-Rust çµ±åˆãƒ©ãƒƒãƒ‘ãƒ¼
Pythonã¨Rustã®å®Ÿè£…ã‚’é€éŽçš„ã«åˆ‡ã‚Šæ›¿ãˆ
"""

import os
import sys

# ç’°å¢ƒå¤‰æ•°ã§å®Ÿè£…ã‚’é¸æŠž
USE_RUST = os.getenv('MUMEI_USE_RUST', 'true').lower() == 'true'

# Rustå®Ÿè£…ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
RUST_AVAILABLE = False
try:
    import mumei_rust
    RUST_AVAILABLE = True
except ImportError:
    pass

# Pythonå®Ÿè£…ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from mm_lexer import Lexer as PythonLexer
    from mm_parser import Parser as PythonParser
    PYTHON_AVAILABLE = True
except ImportError:
    PYTHON_AVAILABLE = False


class MumeiLexer:
    """çµ±åˆãƒ¬ã‚­ã‚µãƒ¼"""

    def __init__(self, source: str):
        self.source = source
        self.use_rust = USE_RUST and RUST_AVAILABLE

    def tokenize(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        if self.use_rust:
            # Rustå®Ÿè£…
            try:
                return mumei_rust.tokenize(self.source)
            except Exception as e:
                print(f"Warning: Rust lexer failed, falling back to Python: {e}")
                self.use_rust = False

        # Pythonå®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if PYTHON_AVAILABLE:
            lexer = PythonLexer(self.source)
            return lexer.tokenize()
        else:
            raise ImportError("No lexer implementation available")


class MumeiParser:
    """çµ±åˆãƒ‘ãƒ¼ã‚µãƒ¼"""

    def __init__(self, source: str = None, tokens=None):
        self.source = source
        self.tokens = tokens
        self.use_rust = USE_RUST and RUST_AVAILABLE and source is not None

    def parse(self):
        """ãƒ‘ãƒ¼ã‚¹"""
        if self.use_rust and self.source:
            # Rustå®Ÿè£…ï¼ˆã‚½ãƒ¼ã‚¹ã‹ã‚‰ç›´æŽ¥ãƒ‘ãƒ¼ã‚¹ï¼‰
            try:
                ast_str = mumei_rust.parse(self.source)
                # æ³¨: ç¾åœ¨ã¯ASTæ–‡å­—åˆ—ã‚’è¿”ã™ã ã‘
                # å°†æ¥çš„ã«ã¯Pythonäº’æ›ã®ASTã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
                return ast_str
            except Exception as e:
                print(f"Warning: Rust parser failed, falling back to Python: {e}")
                self.use_rust = False

        # Pythonå®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if PYTHON_AVAILABLE:
            if self.tokens:
                parser = PythonParser(self.tokens)
            else:
                # ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                lexer = PythonLexer(self.source)
                tokens = lexer.tokenize()
                parser = PythonParser(tokens)
            return parser.parse()
        else:
            raise ImportError("No parser implementation available")


def get_implementation_info():
    """ç¾åœ¨ä½¿ç”¨ä¸­ã®å®Ÿè£…æƒ…å ±ã‚’å–å¾—"""
    info = {
        'rust_available': RUST_AVAILABLE,
        'python_available': PYTHON_AVAILABLE,
        'using_rust': USE_RUST and RUST_AVAILABLE,
        'version': None,
    }

    if RUST_AVAILABLE:
        try:
            info['rust_version'] = mumei_rust.__version__
        except:
            pass

    return info


def print_implementation_info():
    """å®Ÿè£…æƒ…å ±ã‚’å‡ºåŠ›"""
    info = get_implementation_info()

    print("=" * 60)
    print("Mumei Language Implementation")
    print("=" * 60)

    print(f"\nRustå®Ÿè£…:")
    if info['rust_available']:
        print(f"  âœ… åˆ©ç”¨å¯èƒ½")
        if 'rust_version' in info:
            print(f"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {info['rust_version']}")
    else:
        print(f"  âŒ åˆ©ç”¨ä¸å¯ï¼ˆãƒ“ãƒ«ãƒ‰ãŒå¿…è¦: cd mumei-rust && ./build.shï¼‰")

    print(f"\nPythonå®Ÿè£…:")
    if info['python_available']:
        print(f"  âœ… åˆ©ç”¨å¯èƒ½")
    else:
        print(f"  âŒ åˆ©ç”¨ä¸å¯")

    print(f"\nç¾åœ¨ä½¿ç”¨ä¸­:")
    if info['using_rust']:
        print(f"  ðŸ¦€ Rustå®Ÿè£…ï¼ˆé«˜é€Ÿï¼‰")
    elif info['python_available']:
        print(f"  ðŸ Pythonå®Ÿè£…")
    else:
        print(f"  âŒ å®Ÿè£…ãªã—")

    print("\n" + "=" * 60)


# ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯é–¢æ•°
def benchmark_lexer(source: str, iterations: int = 100):
    """ãƒ¬ã‚­ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯"""
    import time

    results = {}

    # Rustå®Ÿè£…
    if RUST_AVAILABLE:
        start = time.time()
        for _ in range(iterations):
            mumei_rust.tokenize(source)
        elapsed = time.time() - start
        results['rust'] = elapsed / iterations

    # Pythonå®Ÿè£…
    if PYTHON_AVAILABLE:
        start = time.time()
        for _ in range(iterations):
            lexer = PythonLexer(source)
            lexer.tokenize()
        elapsed = time.time() - start
        results['python'] = elapsed / iterations

    return results


def print_benchmark(source: str, iterations: int = 100):
    """ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯çµæžœã‚’å‡ºåŠ›"""
    print(f"\nãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ï¼ˆ{iterations}å›žå¹³å‡ï¼‰")
    print(f"ã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {len(source)} æ–‡å­—")
    print("-" * 60)

    results = benchmark_lexer(source, iterations)

    for impl, time_ms in results.items():
        print(f"{impl:10s}: {time_ms * 1000:.2f}ms")

    if 'rust' in results and 'python' in results:
        speedup = results['python'] / results['rust']
        print(f"\né«˜é€ŸåŒ–: {speedup:.1f}x")


if __name__ == "__main__":
    print_implementation_info()

    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
    if len(sys.argv) > 1:
        source = sys.argv[1]
    else:
        source = "let x = 42"

    print(f"\nãƒ†ã‚¹ãƒˆ: {source}")

    if RUST_AVAILABLE or PYTHON_AVAILABLE:
        lexer = MumeiLexer(source)
        tokens = lexer.tokenize()
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")

        # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯
        if len(source) > 10:
            test_source = source * 100
            print_benchmark(test_source, 10)
